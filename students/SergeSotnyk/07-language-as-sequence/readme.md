# Мова як послідовність

## Класифікатор
Дані:

- Виберіть будь-який відкритий корпус та згенеруйте тренувальні дані для моделі. 
Тренувальними даними буде набір склеєних речень. Візьміть до уваги, що склеєних 
речень може бути кілька (зазвичай 2, але буває і 3-4), а перше слово наступного 
речення може писатися з великої чи малої літери.
- Зберіть (чи знайдіть у відкритому доступі) базу енграмів. Візьміть до уваги, 
що відкриті бази енграмів зазвичай містять статистику, зібрану на реченнях, а 
отже вони можуть не містити енграми на межі речень.

Тестування:

- Напишіть бейзлайн та метрику для тестування якості.
- Для тестування використайте корпус 
[run-on-test.json](https://github.com/vseloved/prj-nlp-2019/blob/master/tasks/07-language-as-sequence/run-on-test.json). 

Класифікатор:

- Виділіть ознаки, які впливають на те, чи є слово на межі речень. Подумайте про 
правий/лівий контекст, написання слова, граматичні ознаки (чи може речення 
закінчитись на сполучник?), енграми (чи часто це слово і наступне йдуть поруч?), 
складники та залежності тощо.
- Побудуйте класифікатор на основі логістичної регресії з використанням виділених 
ознак, який анотує послідовно слова у реченні на предмет закінчення речення.
- Спробуйте покращити якість роботи класифікатора, змінюючи набір чи комбінацію ознак.

**Важливо**: оційнюйте якість свого класифікатора в першу чергу на своїх даних 
(train/test або кросвалідація), щоб не підігнати ознаки під надану тестову вибірку.

Запишіть ваші спостереження та результати в окремий файл.

## Хід рішення
### Перевіряємо статистичні характеристики наданого корпусу run-on-test.json

Запускаємо скрипт rot.py:

```
C:\Users\ssotn\Anaconda3\envs\nlp\python.exe D:/git-nlp/ss-prj-nlp-2019/students/SergeSotnyk/07-language-as-sequence/rot.py
Real sentences: 355
Missed ends: 155
Started from lowercase: 80
Total sentences: 200
``` 

### Робимо собі аналогічний корпус для тренування

Один мій однокурсник поділився архівом статей NYT за 2000й рік. Файл NY-Times-2000.zip 
занадто великий для того, щоб я його залив його тут (приблизно 600 МБ), але могу надати 
його особисто.

Утілита **process_nyt2000** вибирає з цього корпусу речення, які закінчуються на крапку 
(під капотом іде очистка корпуса від зайвого тексту за допомогою readability, 
конвертація в текст з якою не всі конвертори добре справляються, та деякі 
ручні маніпуляції для остаточної очистки). На виході маємо файл jsonl (цей формат обрано)
через те, що з json розміром більше гігабайта досить незручно робити), у якому кожна
строка - це набір токенів речення.

```
C:\Users\ssotn\Anaconda3\envs\nlp\python.exe D:/git-nlp/ss-prj-nlp-2019/students/SergeSotnyk/07-language-as-sequence/make_train_dev_ny2000.py
...
Start reading file "data/nyt2000-sents.train.jsonl"
Has read 452465 lines, shuffling in memory
Start writing back
Done!
Start reading file "data/nyt2000-sents.dev.jsonl"
Has read 452525 lines, shuffling in memory
Start writing back
Done!
```

Наступна утілита **make_train_dev_ny2000.py** робить з эталонних речень 
run-on-sentences з приблизно такими ж статистичними особливостями, як і в тестовому
наборі, та розділяє їх на 2 сети - тренувальний, та валідаційний, на якому ми
будемо підбирати налаштування алгоритму. 

```
C:\Users\ssotn\Anaconda3\envs\nlp\python.exe D:/git-nlp/ss-prj-nlp-2019/students/SergeSotnyk/07-language-as-sequence/make_train_dev_ny2000.py
100%|██████████| 6049450/6049450 [01:57<00:00, 51413.51it/s]
Start reading file "data/nyt2000-sents.train.jsonl"
Has read 1320752 lines, shuffling in memory
Start writing back"
Done!
Start reading file "data/nyt2000-sents.dev.jsonl"
Has read 1321908 lines, shuffling in memory
Start writing back"
Done!
```

