{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Task\n",
    "\n",
    "Download and process an arbitrary file from Common Crawl (https://index.commoncrawl.org/), extract individual items, perform basic statistical analysis (distribution of hosts, words, languages, domains etc.) and visualization (optional).\n",
    "\n",
    "# Solution\n",
    "\n",
    "Please, run jupyter notebook in AcquisitionAndAnalysis directory to give access to prjnlp_utils package.\n",
    "\n",
    "First, get the link to warc file manually (we should process one warc only, so we don't need to automate this mechanism), using following steps:\n",
    "\n",
    "0. Open https://index.commoncrawl.org/ page.\n",
    "0. Download cc-index.paths.gz for February 2019\n",
    "0. Unpack it, get the first line 'cc-index/collections/CC-MAIN-2019-09/indexes/cdx-00000.gz'. Add to this filename prefix 'https://commoncrawl.s3.amazonaws.com'\n",
    "0. Download and unpack https://commoncrawl.s3.amazonaws.com/cc-index/collections/CC-MAIN-2019-09/indexes/cdx-00000.gz.\n",
    "0. Find in unpacked cdx-00000 any warc file name you want. I randomly selected crawl-data/CC-MAIN-2019-09/segments/1550247479627.17/warc/CC-MAIN-20190215224408-20190216010408-00052.warc.gz\n",
    "0. Add the same prefix https://commoncrawl.s3.amazonaws.com and get the link to the warc file which we will to process further.\n",
    "\n",
    "https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2019-09/segments/1550247479627.17/warc/CC-MAIN-20190215224408-20190216010408-00052.warc.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "from prjnlp_utils import download_with_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "warc_gz_url: str = \\\n",
    "    'https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2019-09/segments/1550247479627.17/warc/CC-MAIN-20190215224408-20190216010408-00052.warc.gz'\n",
    "archive_name: str = os.path.join(os.getcwd(),\n",
    "                                 'data/CC-MAIN-20190215224408-20190216010408-00052.warc.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'D:\\git-nlp\\ss-prj-nlp-2019\\students\\SergeSotnyk\\03-data\\AcquisitionAndAnalysis\\04-Common-Crawl\\data\\CC-MAIN-20190215224408-20190216010408-00052.warc.gz' is already existed, downloading was skipped.\n"
     ]
    }
   ],
   "source": [
    "# download wark:\n",
    "\n",
    "download_with_progress(warc_gz_url, archive_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from tqdm.auto import tqdm\n",
    "from collections import namedtuple, defaultdict\n",
    "import html2text\n",
    "from selectolax.parser import HTMLParser\n",
    "from textblob import TextBlob\n",
    "import langdetect\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "SinglePageInfo = namedtuple('SinglePageInfo', ['target_uri', 'server', 'language'])\n",
    "\n",
    "tokens = defaultdict(int)\n",
    "langs = defaultdict(int)\n",
    "uris = set()\n",
    "\n",
    "h = html2text.HTML2Text()\n",
    "h.ignore_links = True\n",
    "h.ignore_images = True\n",
    "h.ignore_tables = True\n",
    "h.ignore_emphasis = True\n",
    "\n",
    "def get_text_selectolax(html):\n",
    "    tree = HTMLParser(html)\n",
    "\n",
    "    if tree.body is None:\n",
    "        return None\n",
    "\n",
    "    for tag in tree.css('script'):\n",
    "        tag.decompose()\n",
    "    for tag in tree.css('style'):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = tree.body.text(separator='\\n')\n",
    "    return text\n",
    "\n",
    "def get_text_bs(html):\n",
    "    tree = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    body = tree.body\n",
    "    if body is None:\n",
    "        return None\n",
    "\n",
    "    for tag in body.select('script'):\n",
    "        tag.decompose()\n",
    "    for tag in body.select('style'):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = body.get_text(separator='\\n')\n",
    "    return text\n",
    "\n",
    "def process_part(part):\n",
    "    TARGET_URI_START = 'WARC-Target-URI:'\n",
    "    if len(part) > 1 and part[1].startswith('WARC-Type: response'):\n",
    "        html = ''.join(part)\n",
    "        htmlstart = '<html>'\n",
    "        pos = html.find(htmlstart)\n",
    "        if pos >= 0:\n",
    "            html = html[pos:]\n",
    "            # txt = h.handle(html)\n",
    "            txt = get_text_bs(html)\n",
    "            if not txt:\n",
    "                return\n",
    "            blob = TextBlob(txt)\n",
    "            # blob_lang = blob if len(txt)<2048 else TextBlob(txt[:2048])\n",
    "            try:\n",
    "                lang = langdetect.detect(txt if len(txt)<2048 else txt[:2048])\n",
    "            except:\n",
    "                lang = 'unknown'\n",
    "            langs[lang] += 1\n",
    "            for t in blob.words:\n",
    "                tokens[t] += 1\n",
    "            url = next((l for l in part[:20] if l.startswith(TARGET_URI_START)), None)\n",
    "            if url:\n",
    "                url = url[len(TARGET_URI_START):].strip()\n",
    "                uris.add(url)\n",
    "            \n",
    "\n",
    "def process_warc_gz(filename: str):\n",
    "    lines_counter = 0\n",
    "    res = []\n",
    "    with gzip.open(filename, 'rt', encoding='utf-8', errors='replace') as f:\n",
    "        # 52362938 - it is hardcoded value, I've counted it in the first experiment\n",
    "        part = []\n",
    "\n",
    "        for line in tqdm(f, total=52362938, unit='lines'):\n",
    "            lines_counter += 1\n",
    "            if line.startswith('WARC/1.0'):  # new part started\n",
    "                if part:\n",
    "                    process_part(part)\n",
    "                    # res.append(part)\n",
    "                part = []\n",
    "                # if lines_counter>5000000:\n",
    "                #    break\n",
    "            part.append(line)\n",
    "\n",
    "        if part:  # last part\n",
    "            process_part(part)\n",
    "            # res.append(part)\n",
    "\n",
    "    print(f\"Total lines: {lines_counter}\")\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e85aa658bd843948c8c1dc79faf3329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=52362938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 52362938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load_ext line_profiler\n",
    "# %lprun -f process_part process_warc_gz(archive_name)\n",
    "process_warc_gz(archive_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('en', 2413), ('ru', 611), ('bn', 565), ('de', 307), ('fr', 192), ('ja', 153), ('es', 145), ('unknown', 132), ('ko', 115), ('nl', 87), ('it', 81), ('pl', 79), ('pt', 70), ('cs', 67), ('tr', 56), ('fa', 45), ('sv', 40), ('ca', 32), ('id', 31), ('vi', 30), ('uk', 30), ('ro', 29), ('hu', 27), ('da', 24), ('no', 24), ('fi', 22), ('sk', 20), ('el', 19), ('et', 14), ('bg', 12), ('th', 12), ('sl', 11), ('cy', 11), ('ar', 11), ('tl', 11), ('lt', 10), ('he', 9), ('hr', 9), ('af', 5), ('so', 3), ('lv', 3), ('hi', 2), ('ta', 2)]\n"
     ]
    }
   ],
   "source": [
    "langs_list = sorted([(k, v) for k, v in langs.items() if k.isalnum()], \n",
    "                     key=lambda x: -x[1])\n",
    "print(langs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens 630222\n",
      "Cleared tokens 476255\n",
      "The most frequent tokens:\n",
      "[('a', 968935), ('e', 835190), ('t', 777933), ('i', 677225), ('s', 639936), ('r', 577643), ('o', 559186), ('n', 539051), ('l', 502788), ('d', 471864), ('p', 402999), ('h', 351213), ('c', 341700), ('m', 279178), ('f', 278348), ('1', 270833), ('0', 231168), ('g', 212273), ('2', 202892), ('b', 201289)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total tokens {len(tokens)}\")\n",
    "tokens_list = sorted([(k, v) for k, v in tokens.items() if k.isalnum()], \n",
    "                     key=lambda x: -x[1])\n",
    "print(f\"Cleared tokens {len(tokens_list)}\")\n",
    "print(\"The most frequent tokens:\")\n",
    "print(tokens_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total uries 5889\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total uries {len(uris)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
